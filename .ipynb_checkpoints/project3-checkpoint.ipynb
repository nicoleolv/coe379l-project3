{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab978369-404b-4041-ba09-1aedad219108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14926 images belonging to 2 classes.\n",
      "Found 6396 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(\"data/dnd-cnn-split/train\")\n",
    "    shutil.rmtree(\"data/dnd-cnn-split/test\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ceb9e78-2964-4379-bfce-ffb2d48598cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# ensure directories exist\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"data/dnd-cnn-split/train/damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/dnd-cnn-split/train/no-damage\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(\"data/dnd-cnn-split/test/damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/dnd-cnn-split/test/no-damage\").mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c89f08-7a51-4f0e-8e78-a5d2782ab938",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_damage_file_paths = os.listdir('dataset/Project3/damage')\n",
    "all_no_damage_file_paths = os.listdir('dataset/Project3/no_damage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9426a554-a179-433d-b0d8-8487fd937181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train damage image count:  11336\n",
      "test damage image count:  2834\n",
      "len of overlap:  0\n",
      "train no damage image count:  5721\n",
      "test no damage image count:  1431\n",
      "len of overlap:  0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "train_damage_paths = random.sample(all_damage_file_paths, int(len(all_damage_file_paths)*0.8))\n",
    "print(\"train damage image count: \", len(train_damage_paths))\n",
    "test_damage_paths = [ p for p in all_damage_file_paths if p not in train_damage_paths]\n",
    "print(\"test damage image count: \", len(test_damage_paths))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in train_damage_paths if p in test_damage_paths]\n",
    "print(\"len of overlap: \", len(overlap))\n",
    "\n",
    "\n",
    "train_no_damage_paths = random.sample(all_no_damage_file_paths, int(len(all_no_damage_file_paths)*0.8))\n",
    "print(\"train no damage image count: \", len(train_no_damage_paths))\n",
    "test_no_damage_paths = [ p for p in all_no_damage_file_paths if p not in train_no_damage_paths]\n",
    "print(\"test no damage image count: \", len(test_no_damage_paths))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in train_no_damage_paths if p in test_no_damage_paths]\n",
    "print(\"len of overlap: \", len(overlap))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ec6cd95-9c20-4cd5-be58-8db0a40f8b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in train/damage:  11336\n",
      "Files in train/no-damage:  5721\n",
      "Files in test/damage:  2834\n",
      "Files in test/no-damage:  1431\n"
     ]
    }
   ],
   "source": [
    "# ensure to copy the images to the directories\n",
    "import shutil\n",
    "for p in train_damage_paths:\n",
    "    shutil.copyfile(os.path.join('dataset/Project3/damage', p), os.path.join('data/dnd-cnn-split/train/damage', p) )\n",
    "\n",
    "for p in test_damage_paths:\n",
    "    shutil.copyfile(os.path.join('dataset/Project3/damage', p), os.path.join('data/dnd-cnn-split/test/damage', p) )\n",
    "\n",
    "for p in train_no_damage_paths:\n",
    "    shutil.copyfile(os.path.join('dataset/Project3/no_damage', p), os.path.join('data/dnd-cnn-split/train/no-damage', p) )\n",
    "\n",
    "for p in test_no_damage_paths:\n",
    "    shutil.copyfile(os.path.join('dataset/Project3/no_damage', p), os.path.join('data/dnd-cnn-split/test/no-damage', p) )\n",
    "\n",
    "\n",
    "\n",
    "# check counts:\n",
    "print(\"Files in train/damage: \", len(os.listdir(\"data/dnd-cnn-split/train/damage\")))\n",
    "print(\"Files in train/no-damage: \", len(os.listdir(\"data/dnd-cnn-split/train/no-damage\")))\n",
    "\n",
    "print(\"Files in test/damage: \", len(os.listdir(\"data/dnd-cnn-split/test/damage\")))\n",
    "print(\"Files in test/no-damage: \", len(os.listdir(\"data/dnd-cnn-split/test/no-damage\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0957bff-e5f9-4b95-b5ea-ae0867c4d361",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rescaling\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# path to training data\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "\n",
    "# path to training data\n",
    "train_data_dir = 'data/dnd-cnn-split/train'\n",
    "\n",
    "# controls the size of the \"batches\" of images streamed when accessing the datasets.\n",
    "# this is useful to control the memory usage with very large datasets\n",
    "batch_size = 32\n",
    "\n",
    "# target image size\n",
    "img_height = 150\n",
    "img_width = 150\n",
    "\n",
    "# note that the subset parameter can take values of \"training\", \"validation\", or \"both\";\n",
    "# the value dictates which dataset is returned (we want both)\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "# rescale instance\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "\n",
    "# apply the rescale to the train and validation sets\n",
    "train_rescale_ds = train_ds.map(lambda image,label:(rescale(image),label))\n",
    "val_rescale_ds = val_ds.map(lambda image,label:(rescale(image),label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81bd4b4c-9b76-4e27-8ff4-934493db2449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.8-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Collecting array_record>=0.5.0\n",
      "  Downloading array_record-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dm-tree\n",
      "  Downloading dm_tree-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m296.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting etils[edc,enp,epath,epy,etree]>=1.9.1\n",
      "  Downloading etils-1.12.2-py3-none-any.whl (167 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m303.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting immutabledict\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (1.26.3)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (4.23.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (5.9.8)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Collecting simple_parsing\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.8/112.8 kB\u001b[0m \u001b[31m266.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.17.0-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m289.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m244.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m310.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib_resources\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1->tensorflow_datasets) (4.9.0)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1->tensorflow_datasets) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/site-packages (from dm-tree->tensorflow_datasets) (23.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting docstring-parser<1.0,>=0.15\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4\n",
      "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.2/293.2 kB\u001b[0m \u001b[31m165.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf>=3.20\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m315.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21483 sha256=143abb7e971372b0ea99ff367152bb3d53043a395d528928fb7d96c4817597fa\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-03oem3vw/wheels/74/05/89/d0909dd6ebad0a26f2b4dcb2499b1d65999c5b6ed416be7f85\n",
      "Successfully built promise\n",
      "Installing collected packages: tqdm, toml, pyarrow, protobuf, promise, importlib_resources, immutabledict, fsspec, etils, einops, docstring-parser, dm-tree, simple_parsing, googleapis-common-protos, tensorflow-metadata, array_record, tensorflow_datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.4\n",
      "    Uninstalling protobuf-4.23.4:\n",
      "      Successfully uninstalled protobuf-4.23.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 5.29.4 which is incompatible.\n",
      "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed array_record-0.7.1 dm-tree-0.1.9 docstring-parser-0.16 einops-0.8.1 etils-1.12.2 fsspec-2025.3.2 googleapis-common-protos-1.69.2 immutabledict-4.2.1 importlib_resources-6.5.2 promise-2.3 protobuf-5.29.4 pyarrow-19.0.1 simple_parsing-0.1.7 tensorflow-metadata-1.17.0 tensorflow_datasets-4.9.8 toml-0.10.2 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c58571-6f2e-4522-a661-c77acdd4904e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
